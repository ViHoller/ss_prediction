{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model, optimizers, callbacks, regularizers, layers\n",
    "from keras.layers import Dense, Convolution1D, Dropout, TimeDistributed, Input\n",
    "from ss_functions_alt import *\n",
    "from ss_pred_classes import *\n",
    "\n",
    "path = 'C:/Users/vinicius/Downloads/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1200,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x_train1, x_train2, y_train, lengths\u001b[38;5;241m=\u001b[39m get_data2(path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlist.txt\u001b[39m\u001b[38;5;124m'\u001b[39m,padding_x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m x_train1, x_train2, lengths \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train1\u001b[49m\u001b[43m)\u001b[49m, np\u001b[38;5;241m.\u001b[39marray(x_train2), np\u001b[38;5;241m.\u001b[39marray(lengths)\n\u001b[0;32m      3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1200,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "x_train1, x_train2, y_train, lengths= get_data2(path+'training/','list.txt',padding_x=False)\n",
    "x_train1, x_train2, lengths = np.array(x_train1), np.array(x_train2), np.array(lengths)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, lengths= get_data3(path+'training/','list.txt', padding_x=True,padding_y=True)\n",
    "x_train, lengths, y_train = np.array(x_train), np.array(lengths), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train= get_data(path+'training/','list.txt', padding_x=True,padding_y=True)\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data3(path, file, encode_y=True, padding_x=True, padding_y=True, test=False):\n",
    "    x = []\n",
    "    y = []\n",
    "    lengths = []\n",
    "    with open(path+file, 'r') as sample_file:  # add some stuff to check?\n",
    "        for line in sample_file:\n",
    "            line = line.rstrip()\n",
    "            if test:\n",
    "                line = line.replace(':', '_')\n",
    "            pssm, sequence = parse_pssm(path, line, padding=padding_x)\n",
    "            lengths.append(len(sequence))\n",
    "            sequence_hot = aa_onehot_encoding(sequence, padding=padding_x)\n",
    "            features = np.concatenate((sequence_hot, pssm), axis=1)\n",
    "            x.append(features)\n",
    "\n",
    "            dssp = parse_dssp(path, line).replace('-', 'C')\n",
    "            if encode_y:\n",
    "                dssp = ss_onehot_encoding(dssp, padding=padding_y)\n",
    "\n",
    "            y.append(dssp)\n",
    "    return x, y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "mask = []\n",
    "train_mask = []\n",
    "get, _ = get_data(path+'training/','list.txt', padding_x=False)\n",
    "for i in get:\n",
    "    a = np.zeros((800))\n",
    "    a[:len(i)] = 1\n",
    "    mask.append(tf.expand_dims(a, axis=1))\n",
    "mask = np.array(mask)\n",
    "train_mask = np.array(train_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_monitor_loss = callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=8,\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint = callbacks.ModelCheckpoint(\n",
    "    './ss_pred_modeltest.keras',\n",
    "    monitor='val_weighted_accuracy',\n",
    "    save_best_only=True,\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "decay=0.98\n",
    "def lr_decay(epoch, lr):\n",
    "    return (decay**epoch)*lr\n",
    "\n",
    "lr_schedule = callbacks.LearningRateScheduler(lr_decay)\n",
    "\n",
    "def get_model():\n",
    "    inputs = Input((800,41))\n",
    "    input_mask = Input((800,1))\n",
    "\n",
    "    X = DeepInception_block()(inputs)\n",
    "\n",
    "    X = DeepInception_block()(X)\n",
    "\n",
    "    # X = layers.CuDNNLSTM(512,return_sequences=True)(X)\n",
    "\n",
    "    X = layers.multiply([X, input_mask])\n",
    "    X = layers.Masking(mask_value=0)(X)\n",
    "    X = layers.LSTM(512,return_sequences=True)(X)\n",
    "    # X = Convolution1D(41, 19, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(X)\n",
    "    \n",
    "    # X = layers.add([X, inputs])\n",
    "\n",
    "    X = DeepInception_block()(X)\n",
    "    X = Convolution1D(100, 19, activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.001))(X)\n",
    "\n",
    "    # X = layers.add([X, inputs])\n",
    "    # X = layers.multiply([X, input_mask])\n",
    "    # X = layers.Masking(mask_value=0)(X)\n",
    "    # X = layers.LSTM(512,return_sequences=True)(X)\n",
    "    X = layers.multiply([X, input_mask])\n",
    "    X = layers.Masking(mask_value=0)(X)\n",
    "    X = layers.LSTM(512,return_sequences=True)(X)\n",
    "    X = TimeDistributed(Dense(256, activation='relu'))(X)\n",
    "    X = Dropout(0.4)(X)\n",
    "    Y = TimeDistributed(Dense(3, activation='softmax'))(X)\n",
    "\n",
    "    model = Model(inputs=[inputs, input_mask], outputs=[Y])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "opt = optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss='categorical_crossentropy', # try siome: \"categorical_focal_crossentropy, adam, sparse_categorical_crossentropy\n",
    "              optimizer=opt,\n",
    "              weighted_metrics='accuracy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_49 (InputLayer)          [(None, 800, 41)]    0           []                               \n",
      "                                                                                                  \n",
      " deep_inception_block_56 (DeepI  (None, 800, 369)    234274      ['input_49[0][0]']               \n",
      " nception_block)                                                                                  \n",
      "                                                                                                  \n",
      " deep_inception_block_57 (DeepI  (None, 800, 369)    359242      ['deep_inception_block_56[0][0]']\n",
      " nception_block)                                                                                  \n",
      "                                                                                                  \n",
      " cu_dnnlstm_28 (CuDNNLSTM)      (None, 800, 512)     1808384     ['deep_inception_block_57[0][0]']\n",
      "                                                                                                  \n",
      " input_50 (InputLayer)          [(None, 800, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " multiply_13 (Multiply)         (None, 800, 512)     0           ['cu_dnnlstm_28[0][0]',          \n",
      "                                                                  'input_50[0][0]']               \n",
      "                                                                                                  \n",
      " masking_13 (Masking)           (None, 800, 512)     0           ['multiply_13[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_16 (LSTM)                 (None, 800, 512)     2099200     ['masking_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv1d_4205 (Conv1D)           (None, 800, 41)      398889      ['lstm_16[0][0]']                \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, 800, 41)      0           ['conv1d_4205[0][0]',            \n",
      "                                                                  'input_49[0][0]']               \n",
      "                                                                                                  \n",
      " deep_inception_block_58 (DeepI  (None, 800, 369)    234274      ['add_9[0][0]']                  \n",
      " nception_block)                                                                                  \n",
      "                                                                                                  \n",
      " conv1d_4255 (Conv1D)           (None, 800, 100)     701200      ['deep_inception_block_58[0][0]']\n",
      "                                                                                                  \n",
      " multiply_14 (Multiply)         (None, 800, 100)     0           ['conv1d_4255[0][0]',            \n",
      "                                                                  'input_50[0][0]']               \n",
      "                                                                                                  \n",
      " masking_14 (Masking)           (None, 800, 100)     0           ['multiply_14[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_17 (LSTM)                 (None, 800, 512)     1255424     ['masking_14[0][0]']             \n",
      "                                                                                                  \n",
      " time_distributed_38 (TimeDistr  (None, 800, 256)    131328      ['lstm_17[0][0]']                \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      " dropout_19 (Dropout)           (None, 800, 256)     0           ['time_distributed_38[0][0]']    \n",
      "                                                                                                  \n",
      " time_distributed_39 (TimeDistr  (None, 800, 3)      771         ['dropout_19[0][0]']             \n",
      " ibuted)                                                                                          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,222,986\n",
      "Trainable params: 7,197,894\n",
      "Non-trainable params: 25,092\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 1.3952 - accuracy: 0.4130 - weighted_accuracy: 0.4130\n",
      "Epoch 1: val_weighted_accuracy improved from -inf to 0.42518, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 141s 783ms/step - loss: 1.3952 - accuracy: 0.4130 - weighted_accuracy: 0.4130 - val_loss: 0.6132 - val_accuracy: 0.4252 - val_weighted_accuracy: 0.4252 - lr: 0.0050\n",
      "Epoch 2/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3454 - accuracy: 0.4234 - weighted_accuracy: 0.4234\n",
      "Epoch 2: val_weighted_accuracy did not improve from 0.42518\n",
      "80/80 [==============================] - 48s 596ms/step - loss: 0.3454 - accuracy: 0.4234 - weighted_accuracy: 0.4234 - val_loss: 0.2951 - val_accuracy: 0.4252 - val_weighted_accuracy: 0.4252 - lr: 0.0049\n",
      "Epoch 3/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.4302 - weighted_accuracy: 0.4302\n",
      "Epoch 3: val_weighted_accuracy improved from 0.42518 to 0.42520, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 57s 710ms/step - loss: 0.2551 - accuracy: 0.4302 - weighted_accuracy: 0.4302 - val_loss: 0.3757 - val_accuracy: 0.4252 - val_weighted_accuracy: 0.4252 - lr: 0.0047\n",
      "Epoch 4/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2887 - accuracy: 0.6076 - weighted_accuracy: 0.6076\n",
      "Epoch 4: val_weighted_accuracy improved from 0.42520 to 0.57476, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 51s 632ms/step - loss: 0.2887 - accuracy: 0.6076 - weighted_accuracy: 0.6076 - val_loss: 0.3237 - val_accuracy: 0.5748 - val_weighted_accuracy: 0.5748 - lr: 0.0044\n",
      "Epoch 5/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.3104 - accuracy: 0.6312 - weighted_accuracy: 0.6312\n",
      "Epoch 5: val_weighted_accuracy improved from 0.57476 to 0.62488, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 50s 621ms/step - loss: 0.3104 - accuracy: 0.6312 - weighted_accuracy: 0.6312 - val_loss: 0.3229 - val_accuracy: 0.6249 - val_weighted_accuracy: 0.6249 - lr: 0.0041\n",
      "Epoch 6/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.6734 - weighted_accuracy: 0.6734\n",
      "Epoch 6: val_weighted_accuracy improved from 0.62488 to 0.65276, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 602ms/step - loss: 0.2780 - accuracy: 0.6734 - weighted_accuracy: 0.6734 - val_loss: 0.3079 - val_accuracy: 0.6528 - val_weighted_accuracy: 0.6528 - lr: 0.0037\n",
      "Epoch 7/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2615 - accuracy: 0.6911 - weighted_accuracy: 0.6911\n",
      "Epoch 7: val_weighted_accuracy did not improve from 0.65276\n",
      "80/80 [==============================] - 46s 574ms/step - loss: 0.2615 - accuracy: 0.6911 - weighted_accuracy: 0.6911 - val_loss: 0.3617 - val_accuracy: 0.4231 - val_weighted_accuracy: 0.4231 - lr: 0.0033\n",
      "Epoch 8/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.6950 - weighted_accuracy: 0.6950\n",
      "Epoch 8: val_weighted_accuracy improved from 0.65276 to 0.70059, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 49s 607ms/step - loss: 0.2673 - accuracy: 0.6950 - weighted_accuracy: 0.6950 - val_loss: 0.2787 - val_accuracy: 0.7006 - val_weighted_accuracy: 0.7006 - lr: 0.0028\n",
      "Epoch 9/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2380 - accuracy: 0.6999 - weighted_accuracy: 0.6999\n",
      "Epoch 9: val_weighted_accuracy improved from 0.70059 to 0.70632, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 605ms/step - loss: 0.2380 - accuracy: 0.6999 - weighted_accuracy: 0.6999 - val_loss: 0.2721 - val_accuracy: 0.7063 - val_weighted_accuracy: 0.7063 - lr: 0.0024\n",
      "Epoch 10/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2278 - accuracy: 0.7143 - weighted_accuracy: 0.7143\n",
      "Epoch 10: val_weighted_accuracy improved from 0.70632 to 0.73660, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 52s 645ms/step - loss: 0.2278 - accuracy: 0.7143 - weighted_accuracy: 0.7143 - val_loss: 0.2535 - val_accuracy: 0.7366 - val_weighted_accuracy: 0.7366 - lr: 0.0020\n",
      "Epoch 11/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2203 - accuracy: 0.7219 - weighted_accuracy: 0.7219\n",
      "Epoch 11: val_weighted_accuracy did not improve from 0.73660\n",
      "80/80 [==============================] - 46s 567ms/step - loss: 0.2203 - accuracy: 0.7219 - weighted_accuracy: 0.7219 - val_loss: 0.2254 - val_accuracy: 0.7305 - val_weighted_accuracy: 0.7305 - lr: 0.0016\n",
      "Epoch 12/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2025 - accuracy: 0.7245 - weighted_accuracy: 0.7245\n",
      "Epoch 12: val_weighted_accuracy improved from 0.73660 to 0.75193, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 605ms/step - loss: 0.2025 - accuracy: 0.7245 - weighted_accuracy: 0.7245 - val_loss: 0.2083 - val_accuracy: 0.7519 - val_weighted_accuracy: 0.7519 - lr: 0.0013\n",
      "Epoch 13/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.7365 - weighted_accuracy: 0.7365\n",
      "Epoch 13: val_weighted_accuracy did not improve from 0.75193\n",
      "80/80 [==============================] - 46s 579ms/step - loss: 0.1915 - accuracy: 0.7365 - weighted_accuracy: 0.7365 - val_loss: 0.1991 - val_accuracy: 0.7450 - val_weighted_accuracy: 0.7450 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.7371 - weighted_accuracy: 0.7371\n",
      "Epoch 14: val_weighted_accuracy improved from 0.75193 to 0.76329, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 47s 591ms/step - loss: 0.1804 - accuracy: 0.7371 - weighted_accuracy: 0.7371 - val_loss: 0.1787 - val_accuracy: 0.7633 - val_weighted_accuracy: 0.7633 - lr: 7.9532e-04\n",
      "Epoch 15/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.7477 - weighted_accuracy: 0.7477\n",
      "Epoch 15: val_weighted_accuracy improved from 0.76329 to 0.77017, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 606ms/step - loss: 0.1646 - accuracy: 0.7477 - weighted_accuracy: 0.7477 - val_loss: 0.1707 - val_accuracy: 0.7702 - val_weighted_accuracy: 0.7702 - lr: 5.9939e-04\n",
      "Epoch 16/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.7533 - weighted_accuracy: 0.7533\n",
      "Epoch 16: val_weighted_accuracy improved from 0.77017 to 0.77174, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 49s 608ms/step - loss: 0.1555 - accuracy: 0.7533 - weighted_accuracy: 0.7533 - val_loss: 0.1641 - val_accuracy: 0.7717 - val_weighted_accuracy: 0.7717 - lr: 4.4269e-04\n",
      "Epoch 17/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.7593 - weighted_accuracy: 0.7593\n",
      "Epoch 17: val_weighted_accuracy did not improve from 0.77174\n",
      "80/80 [==============================] - 47s 584ms/step - loss: 0.1493 - accuracy: 0.7593 - weighted_accuracy: 0.7593 - val_loss: 0.1577 - val_accuracy: 0.7695 - val_weighted_accuracy: 0.7695 - lr: 3.2042e-04\n",
      "Epoch 18/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.7581 - weighted_accuracy: 0.7581\n",
      "Epoch 18: val_weighted_accuracy improved from 0.77174 to 0.77482, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 49s 612ms/step - loss: 0.1461 - accuracy: 0.7581 - weighted_accuracy: 0.7581 - val_loss: 0.1535 - val_accuracy: 0.7748 - val_weighted_accuracy: 0.7748 - lr: 2.2728e-04\n",
      "Epoch 19/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1416 - accuracy: 0.7622 - weighted_accuracy: 0.7622\n",
      "Epoch 19: val_weighted_accuracy improved from 0.77482 to 0.77873, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 606ms/step - loss: 0.1416 - accuracy: 0.7622 - weighted_accuracy: 0.7622 - val_loss: 0.1481 - val_accuracy: 0.7787 - val_weighted_accuracy: 0.7787 - lr: 1.5799e-04\n",
      "Epoch 20/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.7662 - weighted_accuracy: 0.7662\n",
      "Epoch 20: val_weighted_accuracy did not improve from 0.77873\n",
      "80/80 [==============================] - 47s 584ms/step - loss: 0.1382 - accuracy: 0.7662 - weighted_accuracy: 0.7662 - val_loss: 0.1468 - val_accuracy: 0.7760 - val_weighted_accuracy: 0.7760 - lr: 1.0763e-04\n",
      "Epoch 21/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1373 - accuracy: 0.7670 - weighted_accuracy: 0.7670\n",
      "Epoch 21: val_weighted_accuracy improved from 0.77873 to 0.77877, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 605ms/step - loss: 0.1373 - accuracy: 0.7670 - weighted_accuracy: 0.7670 - val_loss: 0.1449 - val_accuracy: 0.7788 - val_weighted_accuracy: 0.7788 - lr: 7.1853e-05\n",
      "Epoch 22/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1353 - accuracy: 0.7695 - weighted_accuracy: 0.7695\n",
      "Epoch 22: val_weighted_accuracy improved from 0.77877 to 0.78302, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 606ms/step - loss: 0.1353 - accuracy: 0.7695 - weighted_accuracy: 0.7695 - val_loss: 0.1413 - val_accuracy: 0.7830 - val_weighted_accuracy: 0.7830 - lr: 4.7010e-05\n",
      "Epoch 23/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1342 - accuracy: 0.7699 - weighted_accuracy: 0.7699\n",
      "Epoch 23: val_weighted_accuracy improved from 0.78302 to 0.78338, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 48s 606ms/step - loss: 0.1342 - accuracy: 0.7699 - weighted_accuracy: 0.7699 - val_loss: 0.1404 - val_accuracy: 0.7834 - val_weighted_accuracy: 0.7834 - lr: 3.0142e-05\n",
      "Epoch 24/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1341 - accuracy: 0.7701 - weighted_accuracy: 0.7701\n",
      "Epoch 24: val_weighted_accuracy did not improve from 0.78338\n",
      "80/80 [==============================] - 47s 587ms/step - loss: 0.1341 - accuracy: 0.7701 - weighted_accuracy: 0.7701 - val_loss: 0.1403 - val_accuracy: 0.7830 - val_weighted_accuracy: 0.7830 - lr: 1.8939e-05\n",
      "Epoch 25/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.7740 - weighted_accuracy: 0.7740\n",
      "Epoch 25: val_weighted_accuracy did not improve from 0.78338\n",
      "80/80 [==============================] - 46s 580ms/step - loss: 0.1322 - accuracy: 0.7740 - weighted_accuracy: 0.7740 - val_loss: 0.1403 - val_accuracy: 0.7830 - val_weighted_accuracy: 0.7830 - lr: 1.1663e-05\n",
      "Epoch 26/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1325 - accuracy: 0.7719 - weighted_accuracy: 0.7719\n",
      "Epoch 26: val_weighted_accuracy improved from 0.78338 to 0.78412, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 49s 610ms/step - loss: 0.1325 - accuracy: 0.7719 - weighted_accuracy: 0.7719 - val_loss: 0.1396 - val_accuracy: 0.7841 - val_weighted_accuracy: 0.7841 - lr: 7.0379e-06\n",
      "Epoch 27/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.7714 - weighted_accuracy: 0.7714\n",
      "Epoch 27: val_weighted_accuracy did not improve from 0.78412\n",
      "80/80 [==============================] - 47s 582ms/step - loss: 0.1326 - accuracy: 0.7714 - weighted_accuracy: 0.7714 - val_loss: 0.1395 - val_accuracy: 0.7838 - val_weighted_accuracy: 0.7838 - lr: 4.1622e-06\n",
      "Epoch 28/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.7742 - weighted_accuracy: 0.7742\n",
      "Epoch 28: val_weighted_accuracy did not improve from 0.78412\n",
      "80/80 [==============================] - 47s 584ms/step - loss: 0.1317 - accuracy: 0.7742 - weighted_accuracy: 0.7742 - val_loss: 0.1395 - val_accuracy: 0.7841 - val_weighted_accuracy: 0.7841 - lr: 2.4123e-06\n",
      "Epoch 29/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.7715 - weighted_accuracy: 0.7715\n",
      "Epoch 29: val_weighted_accuracy did not improve from 0.78412\n",
      "80/80 [==============================] - 46s 576ms/step - loss: 0.1330 - accuracy: 0.7715 - weighted_accuracy: 0.7715 - val_loss: 0.1395 - val_accuracy: 0.7839 - val_weighted_accuracy: 0.7839 - lr: 1.3701e-06\n",
      "Epoch 30/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1320 - accuracy: 0.7732 - weighted_accuracy: 0.7732\n",
      "Epoch 30: val_weighted_accuracy improved from 0.78412 to 0.78450, saving model to .\\ss_pred_modeltest.keras\n",
      "80/80 [==============================] - 49s 608ms/step - loss: 0.1320 - accuracy: 0.7732 - weighted_accuracy: 0.7732 - val_loss: 0.1394 - val_accuracy: 0.7845 - val_weighted_accuracy: 0.7845 - lr: 7.6263e-07\n",
      "Epoch 31/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.7731 - weighted_accuracy: 0.7731\n",
      "Epoch 31: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 47s 584ms/step - loss: 0.1322 - accuracy: 0.7731 - weighted_accuracy: 0.7731 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 4.1600e-07\n",
      "Epoch 32/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1331 - accuracy: 0.7721 - weighted_accuracy: 0.7721\n",
      "Epoch 32: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 579ms/step - loss: 0.1331 - accuracy: 0.7721 - weighted_accuracy: 0.7721 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 2.2238e-07\n",
      "Epoch 33/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.7727 - weighted_accuracy: 0.7727\n",
      "Epoch 33: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 576ms/step - loss: 0.1315 - accuracy: 0.7727 - weighted_accuracy: 0.7727 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 1.1650e-07\n",
      "Epoch 34/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.7716 - weighted_accuracy: 0.7716\n",
      "Epoch 34: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 47s 586ms/step - loss: 0.1326 - accuracy: 0.7716 - weighted_accuracy: 0.7716 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 5.9813e-08\n",
      "Epoch 35/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1316 - accuracy: 0.7752 - weighted_accuracy: 0.7752\n",
      "Epoch 35: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 576ms/step - loss: 0.1316 - accuracy: 0.7752 - weighted_accuracy: 0.7752 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 3.0094e-08\n",
      "Epoch 36/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.7739 - weighted_accuracy: 0.7739\n",
      "Epoch 36: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 581ms/step - loss: 0.1315 - accuracy: 0.7739 - weighted_accuracy: 0.7739 - val_loss: 0.1394 - val_accuracy: 0.7841 - val_weighted_accuracy: 0.7841 - lr: 1.4839e-08\n",
      "Epoch 37/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.7723 - weighted_accuracy: 0.7723\n",
      "Epoch 37: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 576ms/step - loss: 0.1323 - accuracy: 0.7723 - weighted_accuracy: 0.7723 - val_loss: 0.1394 - val_accuracy: 0.7842 - val_weighted_accuracy: 0.7842 - lr: 7.1703e-09\n",
      "Epoch 38/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1332 - accuracy: 0.7720 - weighted_accuracy: 0.7720\n",
      "Epoch 38: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 578ms/step - loss: 0.1332 - accuracy: 0.7720 - weighted_accuracy: 0.7720 - val_loss: 0.1395 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 3.3955e-09\n",
      "Epoch 39/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.7728 - weighted_accuracy: 0.7728\n",
      "Epoch 39: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 578ms/step - loss: 0.1327 - accuracy: 0.7728 - weighted_accuracy: 0.7728 - val_loss: 0.1395 - val_accuracy: 0.7841 - val_weighted_accuracy: 0.7841 - lr: 1.5758e-09\n",
      "Epoch 40/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1322 - accuracy: 0.7727 - weighted_accuracy: 0.7727\n",
      "Epoch 40: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 46s 578ms/step - loss: 0.1322 - accuracy: 0.7727 - weighted_accuracy: 0.7727 - val_loss: 0.1394 - val_accuracy: 0.7843 - val_weighted_accuracy: 0.7843 - lr: 7.1665e-10\n",
      "Epoch 41/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.7732 - weighted_accuracy: 0.7732\n",
      "Epoch 41: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 48s 594ms/step - loss: 0.1323 - accuracy: 0.7732 - weighted_accuracy: 0.7732 - val_loss: 0.1394 - val_accuracy: 0.7842 - val_weighted_accuracy: 0.7842 - lr: 3.1941e-10\n",
      "Epoch 42/100\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1327 - accuracy: 0.7720 - weighted_accuracy: 0.7720Restoring model weights from the end of the best epoch: 34.\n",
      "\n",
      "Epoch 42: val_weighted_accuracy did not improve from 0.78450\n",
      "80/80 [==============================] - 45s 560ms/step - loss: 0.1327 - accuracy: 0.7720 - weighted_accuracy: 0.7720 - val_loss: 0.1394 - val_accuracy: 0.7844 - val_weighted_accuracy: 0.7844 - lr: 1.3952e-10\n",
      "Epoch 42: early stopping\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:1'):\n",
    "    history = model.fit([x_train,mask], y_train,\n",
    "                        epochs=100,\n",
    "                        batch_size=12,\n",
    "                        validation_split=0.2,\n",
    "                        sample_weight=mask,\n",
    "                        callbacks=[stop_monitor_loss, checkpoint, lr_schedule])\n",
    "    \n",
    "# 3rd block - negligeable improvement\n",
    "# mask - did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model('ss_pred_modeltest.keras', \n",
    "                                     custom_objects={\n",
    "                                         'inception_conv': inception_conv,\n",
    "                                         'InceptionNet_paper': InceptionNet_paper,\n",
    "                                         'DeepInception_block': DeepInception_block,\n",
    "                                         'truncated_accuracy': truncated_accuracy\n",
    "                                     })\n",
    "# test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = get_data(path+'blindTest/','list.txt',encode_y=False, padding_x=True, padding_y=False, test=True)\n",
    "x_test = np.array(x_test)\n",
    "input_mask_test = []\n",
    "get, _ = get_data(path+'blindTest/','list.txt', padding_x=False, test=True)\n",
    "for i in get:\n",
    "    a = np.zeros((800))\n",
    "    a[:len(i)] = 1\n",
    "    input_mask_test.append(tf.expand_dims(a, axis=1))\n",
    "input_mask_test = np.array(input_mask_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/11 [==============================] - 2s 171ms/step\n",
      "0.7657718552851809\n"
     ]
    }
   ],
   "source": [
    "ss_map = {'C': 0, 'H': 1, 'E': 2}\n",
    "from_aa = {0: 'C', 1: 'H', 2: 'E'}\n",
    "predictions_hot = test_model.predict((x_test, input_mask_test))\n",
    "predictions = []\n",
    "for prediction in predictions_hot:\n",
    "    dssp = ''\n",
    "    for i in prediction:\n",
    "        dssp += from_aa[np.argmax(i)]\n",
    "    predictions.append(dssp)\n",
    "\n",
    "total = 0\n",
    "TP = 0\n",
    "for prediction, truth in zip(predictions, y_test):\n",
    "    for i, ss in enumerate(truth):\n",
    "        total +=1\n",
    "        if ss==prediction[i]:\n",
    "            TP+=1\n",
    "\n",
    "accuracy = TP/total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test1, x_test2, y_test, lengths = get_data2(path+'blindTest/','list.txt',encode_y=False, padding_x=True, padding_y=False, test=True)\n",
    "input_mask_test = []\n",
    "get, _ = get_data(path+'blindTest/','list.txt', padding_x=False, test=True)\n",
    "for i in get:\n",
    "    a = np.zeros((800))\n",
    "    a[:len(i)] = 1\n",
    "    input_mask_test.append(tf.expand_dims(a, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model_2\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 800, 20) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 800, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 800, 1) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n",
      "\u001b[0;32m      1\u001b[0m ss_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m}\n",
      "\u001b[0;32m      2\u001b[0m from_aa \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[1;32m----> 3\u001b[0m predictions_hot \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mask_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m      4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prediction \u001b[38;5;129;01min\u001b[39;00m predictions_hot:\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n",
      "\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n",
      "\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file4aye0op0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n",
      "\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n",
      "\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"c:\\Users\\vinicius\\anaconda3\\envs\\tf\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n",
      "        raise ValueError(\n",
      "\n",
      "    ValueError: Layer \"model_2\" expects 1 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 800, 20) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 800, 21) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 800, 1) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "ss_map = {'C': 0, 'H': 1, 'E': 2}\n",
    "from_aa = {0: 'C', 1: 'H', 2: 'E'}\n",
    "predictions_hot = test_model.predict([np.array(x_test1),np.array(x_test2), np.array(input_mask_test)])\n",
    "predictions = []\n",
    "for prediction in predictions_hot:\n",
    "    dssp = ''\n",
    "    for i in prediction:\n",
    "        dssp += from_aa[np.argmax(i)]\n",
    "    predictions.append(dssp)\n",
    "\n",
    "total = 0\n",
    "TP = 0\n",
    "for prediction, truth in zip(predictions, y_test[:200]):\n",
    "    for i, ss in enumerate(truth):\n",
    "        total +=1\n",
    "        if ss==prediction[i]:\n",
    "            TP+=1\n",
    "\n",
    "accuracy = TP/total\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input:\n",
    "1. one hot encoded sequence\n",
    "2. PSSM\n",
    "\n",
    "Model:\n",
    "1D convolutional neural network\n",
    "\n",
    "output:\n",
    "multiclass classification - dense layer with relu activaiton - 3?\n",
    "\n",
    "validation metric - accuray + model specific measures\n",
    "\n",
    "soruces:\n",
    "https://www.csbj.org/article/S2001-0370(22)00506-2/fulltext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
